# LiquidAI VLM Model Configuration
# https://huggingface.co/LiquidAI/LFM2.5-VL-1.6B
#
# LOADING REQUIREMENT:
#   LiquidAI models use a custom architecture (`lfm2_vl`) that requires
#   Unsloth's FastVisionModel for loading (NOT standard transformers).
#   
#   See: https://github.com/Liquid4All/cookbook
#   
# INSTALL:
#   pip install unsloth
#   # Or for Colab: pip install --no-deps bitsandbytes accelerate xformers peft trl triton unsloth_zoo
#
# HARDWARE REQUIREMENTS:
#   - NVIDIA GPU with CUDA 11.8+ (recommended)
#   - Also works on Apple Silicon (MPS) and CPU
#   - Minimum 8GB VRAM / 16GB RAM

model_id: "LiquidAI/LFM2.5-VL-1.6B"
model_name: "liquidai-ocr"
loader: "unsloth"  # Requires Unsloth's FastVisionModel

# Hardware profile
hardware:
  # Optimal platform for this model
  optimal: "cuda"
  # Supported platforms (in order of preference)
  supported:
    - platform: "cuda"
      attn_implementation: "sdpa"
      notes: "Best performance"
    - platform: "mps"  # Apple Silicon
      attn_implementation: "sdpa"
      notes: "Good performance on M1/M2/M3"
    - platform: "cpu"
      attn_implementation: "eager"
      notes: "Slow but works for testing"
  # Minimum requirements
  min_vram_gb: 4
  recommended_vram_gb: 8
  min_ram_gb: 8

# Alternative models for different hardware
alternatives:
  # For very limited resources
  - condition: "ram < 8"
    model_id: "microsoft/trocr-small-printed"
    model_name: "trocr-small-fallback"
    notes: "Lightweight OCR model for limited RAM"

# Dependency requirements
requirements:
  python: ">=3.9"
  transformers: ">=4.40.0"
  torch: ">=2.0.0"

# Fallback configuration when model fails to load
fallback:
  enabled: true
  type: "baseline"
  model_name: "liquidai-ocr-baseline"
  reason: "Fallback for dependency or loading issues"

# Training defaults
training:
  trust_remote_code: true
  use_safetensors: true
  prompt: ""
  max_new_tokens: 32
  attn_implementation: "auto"
